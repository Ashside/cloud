import os
import sys
import subprocess
import threading
import json
import socket
import atexit
import signal
import re
import uuid
import urllib.parse
from flask import Flask, render_template, request, jsonify, redirect, url_for, send_file
from flask import g
import time
import psutil
import glob
import pathlib
import requests
from werkzeug.utils import secure_filename
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

# å¤ç”¨evalé€»è¾‘
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import apply_lora, load_lora
from trainer.trainer_utils import setup_seed

# å°è¯•å¯¼å…¥torchæ¥æ£€æµ‹GPU
try:
    import torch
    HAS_TORCH = True
    # æ£€æµ‹å¯ç”¨çš„GPUæ•°é‡å’Œè®¾å¤‡ä¿¡æ¯
    if torch.cuda.is_available():
        GPU_COUNT = torch.cuda.device_count()
        # è·å–GPUè®¾å¤‡åç§°
        GPU_NAMES = [torch.cuda.get_device_name(i) for i in range(GPU_COUNT)]
    else:
        GPU_COUNT = 0
        GPU_NAMES = []
except ImportError:
    HAS_TORCH = False
    GPU_COUNT = 0
    GPU_NAMES = []

def calculate_training_progress(process_id, process_info):
    """
    è®¡ç®—è®­ç»ƒè¿›åº¦ä¿¡æ¯
    ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–è®­ç»ƒè¿›åº¦ã€lossã€epochç­‰ä¿¡æ¯
    """
    progress = {
        'percentage': 0,
        'current_epoch': 0,
        'total_epochs': 0,
        'current_step': 0,
        'total_steps': 0,
        'remaining_time': 'è®¡ç®—ä¸­...',
        'current_loss': None,
        'current_lr': None
    }
    
    # å¦‚æœè¿›ç¨‹ä¸åœ¨è¿è¡Œä¸”æ²¡æœ‰æ—¥å¿—æ–‡ä»¶ï¼Œè¿”å›ç©ºè¿›åº¦
    if not process_info.get('running', False):
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥å¿—æ–‡ä»¶ï¼Œå¦‚æœæœ‰åˆ™ç»§ç»­è§£æ
        script_dir = os.path.dirname(os.path.abspath(__file__))
        log_dir = os.path.join(script_dir, '../logfile')
        log_dir = os.path.abspath(log_dir)
        
        log_file_exists = False
        if os.path.exists(log_dir):
            for filename in os.listdir(log_dir):
                if filename.endswith(f'{process_id}.log'):
                    log_file_exists = True
                    break
        
        # å¦‚æœæ²¡æœ‰æ—¥å¿—æ–‡ä»¶ä¸”è¿›ç¨‹ä¸åœ¨è¿è¡Œï¼Œè¿”å›ç©ºè¿›åº¦
        if not log_file_exists:
            return progress
    
    try:
        # è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        log_dir = os.path.join(script_dir, '../logfile')
        log_dir = os.path.abspath(log_dir)
        
        log_file = None
        if os.path.exists(log_dir):
            for filename in os.listdir(log_dir):
                if filename.endswith(f'{process_id}.log'):
                    log_file = os.path.join(log_dir, filename)
                    break
        
        if not log_file or not os.path.exists(log_file):
            return progress
        
        # è¯»å–æ—¥å¿—æ–‡ä»¶çš„æœ€å1000è¡Œ
        def read_last_lines(file_path, n=1000):
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•è¯»å–æœ€ånè¡Œ
                    lines = []
                    for line in f:
                        lines.append(line.strip())
                        if len(lines) > n:
                            lines.pop(0)
                    return lines
            except Exception:
                return []
        
        lines = read_last_lines(log_file, 1000)
        
        # ä»æ—¥å¿—ä¸­æå–è¿›åº¦ä¿¡æ¯
        current_epoch = 0
        total_epochs = 0
        current_loss = None
        current_lr = None
        
        for line in reversed(lines):  # ä»æœ€æ–°æ—¥å¿—å¼€å§‹
            line = line.strip()
            if not line:
                continue
                
            # æå–epochä¿¡æ¯ - æ”¯æŒå¤šç§æ ¼å¼
            if not total_epochs:
                # æ ¼å¼: epoch 3/10, Epoch 3 of 10, [3/10], ç¬¬3è½®/å…±10è½®, Epoch:[1/1]
                epoch_patterns = [
                    r'Epoch:\[(\d+)/(\d+)\]',                      # Epoch:[1/1] - æ–°æ ¼å¼
                    r'epoch\s+(\d+)\s*/\s*(\d+)',
                    r'Epoch\s+(\d+)\s*of\s*(\d+)',
                    r'\[(\d+)/(\d+)\]',
                    r'epoch\s*[:ï¼š]\s*(\d+)\s*/\s*(\d+)',
                    r'ç¬¬\s*(\d+)\s*è½®\s*/\s*å…±\s*(\d+)\s*è½®'
                ]
                
                for pattern in epoch_patterns:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        if r'Epoch:\[' in pattern:
                            current_epoch = int(match.group(1))
                            total_epochs = int(match.group(2))
                        else:
                            current_epoch = int(match.group(1))
                            total_epochs = int(match.group(2))
                        break
            
            # æå–stepä¿¡æ¯ - æ”¯æŒå¤šç§æ ¼å¼
            # æ ¼å¼: (74/44160), step 150/1000, Step 150 of 1000, [150/1000], step: 150/1000
            step_patterns = [
                r'\((\d+)/(\d+)\)',                            # (74/44160) - æ–°æ ¼å¼
                r'step\s+(\d+)\s*/\s*(\d+)',
                r'Step\s+(\d+)\s*of\s*(\d+)',
                r'\[(\d+)/(\d+)\]',
                r'step\s*[:ï¼š]\s*(\d+)\s*/\s*(\d+)',
                r'ç¬¬\s*(\d+)\s*æ­¥\s*/\s*å…±\s*(\d+)\s*æ­¥',
                r'æ­¥æ•°\s*(\d+)\s*/\s*(\d+)',
                r'batch\s+(\d+)\s*/\s*(\d+)',  # ä¹Ÿæ”¯æŒbatchæ ¼å¼
                r'Batch\s+(\d+)\s*of\s*(\d+)'
            ]
            
            for pattern in step_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    progress['current_step'] = int(match.group(1))
                    progress['total_steps'] = int(match.group(2))
                    break
            
            # æå–lossä¿¡æ¯ - æ”¯æŒå¤šç§æ ¼å¼
            if not current_loss:
                # æ ¼å¼: loss:8.896761, loss: 4.32, training_loss: 4.32, train_loss: 4.32, Loss: 4.32, è®­ç»ƒæŸå¤±: 4.32
                loss_patterns = [
                    r'loss:([\d.]+(?:e[+-]?\d+)?)',                    # loss:8.896761 - æ–°æ ¼å¼
                    r'loss[\s:=]\s*([\d.]+(?:e[+-]?\d+)?)',           # loss: 4.32
                    r'training_loss[\s:=]\s*([\d.]+(?:e[+-]?\d+)?)',  # training_loss: 4.32
                    r'train_loss[\s:=]\s*([\d.]+(?:e[+-]?\d+)?)',     # train_loss: 4.32
                    r'Loss[\s:=]\s*([\d.]+(?:e[+-]?\d+)?)',          # Loss: 4.32
                    r'è®­ç»ƒæŸå¤±[\s:=]\s*([\d.]+(?:e[+-]?\d+)?)',        # è®­ç»ƒæŸå¤±: 4.32
                    r'æŸå¤±[\s:=]\s*([\d.]+(?:e[+-]?\d+)?)',           # æŸå¤±: 4.32
                    r'\s+([\d.]+(?:e[+-]?\d+)?)\s*loss',             # 4.32 loss
                    r'\s+([\d.]+(?:e[+-]?\d+)?)\s*è®­ç»ƒæŸå¤±',           # 4.32 è®­ç»ƒæŸå¤±
                    r'(?:loss|æŸå¤±|training_loss|train_loss)\s*=\s*([\d.]+(?:e[+-]?\d+)?)'  # loss = 4.32
                ]
                
                for pattern in loss_patterns:
                    matches = re.findall(pattern, line, re.IGNORECASE)
                    if matches:
                        # å–æœ€åä¸€ä¸ªåŒ¹é…çš„losså€¼
                        loss_value = float(matches[-1])
                        if 0 < loss_value < 100:  # åˆç†çš„lossèŒƒå›´
                            current_loss = loss_value
                            break
            
            # æå–å­¦ä¹ ç‡ä¿¡æ¯ - æ”¯æŒå¤šç§æ ¼å¼
            if not current_lr:
                # æ ¼å¼: lr:0.000549999999, lr: 1e-4, learning_rate: 1e-4, LR: 1e-4, å­¦ä¹ ç‡: 1e-4
                lr_patterns = [
                    r'lr:([\d.e+-]+)',                              # lr:0.000549999999 - æ–°æ ¼å¼
                    r'lr[\s:=]\s*([\d.e+-]+)',
                    r'learning_rate[\s:=]\s*([\d.e+-]+)',
                    r'LR[\s:=]\s*([\d.e+-]+)',
                    r'å­¦ä¹ ç‡[\s:=]\s*([\d.e+-]+)'
                ]
                
                for pattern in lr_patterns:
                    matches = re.findall(pattern, line, re.IGNORECASE)
                    if matches:
                        # å–æœ€åä¸€ä¸ªåŒ¹é…çš„lrå€¼
                        lr_value = float(matches[-1])
                        if 0 < lr_value < 1:  # åˆç†çš„lrèŒƒå›´
                            current_lr = f"{lr_value:.2e}"
                            break
            
            # å¦‚æœå·²ç»æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œæå‰é€€å‡º
            if total_epochs and current_loss and current_lr:
                break
        
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯” - æ”¯æŒepochå’ŒstepåŒé‡è¿›åº¦
        percentage = 0
        if total_epochs > 0:
            # åŸºç¡€epochè¿›åº¦
            epoch_percentage = (current_epoch / total_epochs) * 100
            
            # å¦‚æœæœ‰stepä¿¡æ¯ï¼Œåœ¨å½“å‰epochå†…è®¡ç®—stepè¿›åº¦
            if progress['total_steps'] > 0 and progress['current_step'] > 0:
                # è®¡ç®—å½“å‰epochå†…çš„stepè¿›åº¦
                step_percentage_in_epoch = (progress['current_step'] / progress['total_steps']) * 100
                # å°†stepè¿›åº¦åŠ åˆ°epochè¿›åº¦ä¸Šï¼ˆæ¯ä¸ªepochå æ€»è¿›åº¦çš„1/total_epochsï¼‰
                step_contribution = step_percentage_in_epoch / total_epochs
                percentage = min(100, max(0, int(epoch_percentage + step_contribution)))
            else:
                # åªæœ‰epochä¿¡æ¯çš„ä¼ ç»Ÿè®¡ç®—æ–¹å¼
                percentage = min(100, max(0, int(epoch_percentage)))
        
        # æ›´æ–°è¿›åº¦å­—å…¸
        progress['percentage'] = percentage
        progress['current_epoch'] = current_epoch
        progress['total_epochs'] = total_epochs
        progress['current_loss'] = current_loss
        progress['current_lr'] = current_lr
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´ï¼ˆå¢å¼ºè®¡ç®—ï¼‰
        remaining_time = 'è®¡ç®—ä¸­...'
        if current_epoch > 0 and total_epochs > current_epoch:
            # ä»æ—¥å¿—ä¸­æå–æ—¶é—´ä¿¡æ¯
            for line in reversed(lines):
                # æ ¼å¼: remaining: 1:30:45, ETA: 1:30:45, é¢„è®¡å‰©ä½™: 1å°æ—¶30åˆ†é’Ÿ, epoch_Time:332.0min:
                time_patterns = [
                    r'epoch_Time:([\d.]+)min:',                    # epoch_Time:332.0min: - æ–°æ ¼å¼
                    r'remaining[\s:=]\s*(\d+):(\d+):(\d+)',      # remaining: 1:30:45
                    r'ETA[\s:=]\s*(\d+):(\d+):(\d+)',            # ETA: 1:30:45
                    r'é¢„è®¡å‰©ä½™[\s:=]\s*(\d+)[\så°æ—¶]*[\s:]?(\d+)?[\såˆ†é’Ÿ]*',  # é¢„è®¡å‰©ä½™: 1å°æ—¶30åˆ†é’Ÿ
                    r'å‰©ä½™æ—¶é—´[\s:=]\s*(\d+)[\så°æ—¶]*[\s:]?(\d+)?[\såˆ†é’Ÿ]*',  # å‰©ä½™æ—¶é—´: 1å°æ—¶30åˆ†é’Ÿ
                    r'time left[\s:=]\s*(\d+)[\s:]?(\d+)?[\s:]?(\d+)?',  # time left: 1:30:45
                    r'è¿˜éœ€[\s:=]\s*(\d+)[\så°æ—¶]*[\s:]?(\d+)?[\såˆ†é’Ÿ]*'  # è¿˜éœ€: 1å°æ—¶30åˆ†é’Ÿ
                ]
                
                for pattern in time_patterns:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        # å¤„ç†epoch_Timeæ ¼å¼
                        if 'epoch_Time:' in pattern:
                            minutes = float(match.group(1))
                            if minutes > 0:
                                if minutes >= 60:
                                    hours = int(minutes // 60)
                                    remaining_minutes = int(minutes % 60)
                                    if hours > 0:
                                        remaining_time = f"{hours}å°æ—¶{remaining_minutes}åˆ†é’Ÿ"
                                    else:
                                        remaining_time = f"{remaining_minutes}åˆ†é’Ÿ"
                                else:
                                    remaining_time = f"{int(minutes)}åˆ†é’Ÿ"
                                break
                        else:
                            groups = match.groups()
                            if len(groups) >= 3 and all(groups[:3]):
                                # å°æ—¶:åˆ†é’Ÿ:ç§’æ ¼å¼
                                hours = int(groups[0])
                                minutes = int(groups[1])
                                seconds = int(groups[2])
                                if hours > 0 or minutes > 0 or seconds > 0:
                                    parts = []
                                    if hours > 0: parts.append(f"{hours}å°æ—¶")
                                    if minutes > 0: parts.append(f"{minutes}åˆ†é’Ÿ")
                                    if seconds > 0 and hours == 0 and minutes == 0:
                                        parts.append(f"{seconds}ç§’")
                                    remaining_time = ''.join(parts)
                                    break
                            elif len(groups) >= 2:
                                # å°æ—¶å’Œåˆ†é’Ÿæ ¼å¼
                                hours = int(groups[0])
                                minutes = int(groups[1]) if groups[1] else 0
                                if hours > 0 or minutes > 0:
                                    parts = []
                                    if hours > 0: parts.append(f"{hours}å°æ—¶")
                                    if minutes > 0: parts.append(f"{minutes}åˆ†é’Ÿ")
                                    remaining_time = ''.join(parts)
                                    break
                
                if remaining_time != 'è®¡ç®—ä¸­...':
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´ä¿¡æ¯ï¼Œæ ¹æ®è¿›åº¦ä¼°ç®—
            if remaining_time == 'è®¡ç®—ä¸­...':
                # å‡è®¾æ¯epochæ—¶é—´å¤§è‡´ç›¸åŒ
                elapsed_time = time.time() - process_info.get('start_timestamp', time.time())
                if current_epoch > 0:
                    time_per_epoch = elapsed_time / current_epoch
                    remaining_epochs = total_epochs - current_epoch
                    remaining_seconds = remaining_epochs * time_per_epoch
                    
                    if remaining_seconds > 3600:
                        remaining_time = f"{remaining_seconds / 3600:.1f}å°æ—¶"
                    elif remaining_seconds > 60:
                        remaining_time = f"{remaining_seconds / 60:.1f}åˆ†é’Ÿ"
                    else:
                        remaining_time = f"{int(remaining_seconds)}ç§’"
        
        return {
            'percentage': percentage,
            'current_epoch': current_epoch,
            'total_epochs': total_epochs,
            'current_step': progress['current_step'],
            'total_steps': progress['total_steps'],
            'remaining_time': remaining_time,
            'current_loss': f"{current_loss:.4f}" if current_loss else None,
            'current_lr': current_lr
        }
        
    except Exception as e:
        print(f"è®¡ç®—è¿›åº¦æ—¶å‡ºé”™: {e}")
        return progress

# è®­ç»ƒæ–¹å¼æ”¯æŒæ£€æµ‹
def get_supported_training_methods():
    """è·å–å½“å‰ç¯å¢ƒæ”¯æŒçš„è®­ç»ƒæ–¹æ³•"""
    methods = {
        'pretrain': True,  # é¢„è®­ç»ƒæ€»æ˜¯æ”¯æŒ
        'sft': True,       # SFTæ€»æ˜¯æ”¯æŒ
        'lora': True,      # LoRAæ€»æ˜¯æ”¯æŒ
        'dpo': True,       # DPOæ€»æ˜¯æ”¯æŒ
        'multi_gpu': HAS_TORCH and GPU_COUNT > 1  # å¤šGPUè®­ç»ƒéœ€è¦PyTorchå’Œå¤šä¸ªGPU
    }
    return methods

# è·å–å½“å‰ç¯å¢ƒæ”¯æŒçš„è®­ç»ƒæ–¹æ³•
SUPPORTED_METHODS = get_supported_training_methods()

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

app = Flask(__name__, template_folder='templates', static_folder='static')

# å­˜å‚¨è®­ç»ƒè¿›ç¨‹çš„ä¿¡æ¯
training_processes = {}

# è¿›ç¨‹ä¿¡æ¯æŒä¹…åŒ–æ–‡ä»¶
PROCESSES_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'training_processes.json')

# PIDæ–‡ä»¶
PID_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'train_web_ui.pid')

# é¡¹ç›®ä¸ä¼ è¾“ç›¸å…³çš„è·¯å¾„/é…ç½®
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..'))
OUT_DIR = os.path.abspath(os.path.join(PROJECT_ROOT, 'out'))
DATASET_DIR = os.path.abspath(os.path.join(PROJECT_ROOT, 'dataset'))
TRANSFER_TOKEN = os.environ.get('TRANSFER_TOKEN')  # å¯é€‰ï¼šç”¨äºç®€å•é‰´æƒ
ALLOWED_PORTS = [6006, 6008]  # autodl ä»…å¼€æ”¾ç«¯å£
TRANSFER_DEBUG = os.environ.get('TRANSFER_DEBUG', '0') == '1'

# è·¨æœåŠ¡å™¨ä¼ è¾“ä»»åŠ¡çŠ¶æ€
transfer_tasks = {}
transfer_lock = threading.Lock()

# LoRA è¿œç¨‹åä½œä»»åŠ¡çŠ¶æ€
lora_exchange_tasks = {}
lora_lock = threading.Lock()


def ensure_out_dir():
    os.makedirs(OUT_DIR, exist_ok=True)
    return OUT_DIR


def ensure_dataset_dir():
    os.makedirs(DATASET_DIR, exist_ok=True)
    return DATASET_DIR


def normalize_target_url(raw_url):
    """æ ‡å‡†åŒ–ç›®æ ‡URLï¼Œç¼ºå¤±åè®®æ—¶é»˜è®¤http"""
    if not raw_url:
        return None
    if isinstance(raw_url, str):
        raw_url = raw_url.strip()
    if not raw_url:
        return None
    parsed = urllib.parse.urlparse(raw_url)
    if not parsed.scheme:
        raw_url = f"http://{raw_url}"
        parsed = urllib.parse.urlparse(raw_url)
    if not parsed.netloc:
        return None
    # autodl ä»…å¼€æ”¾ 6006/6008ï¼Œè‹¥æœªæŒ‡å®šç«¯å£åˆ™é»˜è®¤ 6006
    if parsed.port is None and parsed.hostname:
        default_port = ALLOWED_PORTS[0]
        netloc = f"{parsed.hostname}:{default_port}"
        parsed = parsed._replace(netloc=netloc)
    # ä»…ä¿ç•™ scheme://netloc ä»¥åŠ pathï¼ˆå»æ‰æœ«å°¾çš„æ–œæ ï¼‰
    normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')
    return normalized


def safe_out_path(rel_path):
    """å°†ç›¸å¯¹è·¯å¾„é™åˆ¶åœ¨outç›®å½•ä¸‹ï¼Œå¹¶è¿”å›(å®‰å…¨ç›¸å¯¹è·¯å¾„, ç»å¯¹è·¯å¾„)"""
    if not rel_path:
        return None, None
    normalized = os.path.normpath(rel_path).replace('\\', '/')
    normalized = normalized.lstrip('/')
    if normalized.startswith('..'):
        return None, None
    parts = [secure_filename(p) for p in normalized.split('/') if p not in ('', '.')]
    safe_rel = '/'.join([p for p in parts if p])
    if not safe_rel:
        return None, None
    ensure_out_dir()
    abs_path = os.path.abspath(os.path.join(OUT_DIR, *safe_rel.split('/')))
    if not abs_path.startswith(OUT_DIR):
        return None, None
    return safe_rel, abs_path


def safe_dataset_path(rel_path):
    """å°†ç›¸å¯¹è·¯å¾„é™åˆ¶åœ¨datasetç›®å½•ä¸‹ï¼Œå¹¶è¿”å›(å®‰å…¨ç›¸å¯¹è·¯å¾„, ç»å¯¹è·¯å¾„)"""
    if not rel_path:
        return None, None
    normalized = os.path.normpath(rel_path).replace('\\', '/')
    normalized = normalized.lstrip('/')
    if normalized.startswith('..'):
        return None, None
    parts = [secure_filename(p) for p in normalized.split('/') if p not in ('', '.')]
    safe_rel = '/'.join([p for p in parts if p])
    if not safe_rel:
        return None, None
    ensure_dataset_dir()
    abs_path = os.path.abspath(os.path.join(DATASET_DIR, *safe_rel.split('/')))
    if not abs_path.startswith(DATASET_DIR):
        return None, None
    return safe_rel, abs_path


def parse_weight_name(filename):
    """è§£ææƒé‡æ–‡ä»¶åï¼Œè¿”å›å‰ç¼€/hidden_size/moeæ ‡è®°"""
    base = os.path.basename(filename)
    match = re.match(r'(.+?)_(\d+)(?:_moe)?\.pth$', base)
    if not match:
        return None
    name = match.group(1)
    hidden = int(match.group(2))
    is_moe = base.endswith('_moe.pth')
    return {'name': name, 'hidden_size': hidden, 'use_moe': 1 if is_moe else 0}


def list_weights():
    ensure_out_dir()
    base_weights = []
    for root, _, files in os.walk(OUT_DIR):
        # è·³è¿‡LoRAå­ç›®å½•
        if os.path.basename(root) == 'lora':
            continue
        for f in files:
            if not f.endswith('.pth'):
                continue
            info = parse_weight_name(f)
            if not info:
                continue
            rel = os.path.relpath(os.path.join(root, f), OUT_DIR)
            base_weights.append({**info, 'relative_path': rel})
    base_weights.sort(key=lambda x: x['relative_path'])

    lora_dir = os.path.join(OUT_DIR, 'lora')
    lora_weights = []
    if os.path.exists(lora_dir):
        for f in os.listdir(lora_dir):
            if not f.endswith('.pth'):
                continue
            info = parse_weight_name(f)
            if not info:
                continue
            lora_weights.append({**info, 'relative_path': os.path.join('lora', f)})
        lora_weights.sort(key=lambda x: x['relative_path'])

    return base_weights, lora_weights


def check_transfer_token(req, allow_body=False):
    """å½“è®¾ç½®äº†TRANSFER_TOKENæ—¶ï¼Œæ ¡éªŒä¼ è¾“ä»¤ç‰Œ"""
    if not TRANSFER_TOKEN:
        return None
    provided = req.headers.get('X-Transfer-Token') or req.args.get('token')
    if not provided and allow_body and req.is_json:
        provided = (req.get_json(silent=True) or {}).get('token')
    if provided != TRANSFER_TOKEN:
        return jsonify({'error': 'æ— æ•ˆçš„ä¼ è¾“ä»¤ç‰Œ'}), 401
    return None


def create_transfer_task(task_type, filename, endpoint=None):
    task_id = uuid.uuid4().hex
    with transfer_lock:
        transfer_tasks[task_id] = {
            'id': task_id,
            'type': task_type,
            'filename': filename,
            'endpoint': endpoint,
            'status': 'pending',
            'progress': 0,
            'message': 'å¾…å¼€å§‹'
        }
        # ä¿ç•™æœ€è¿‘çš„ä»»åŠ¡ï¼Œé¿å…æ— é™å¢é•¿
        if len(transfer_tasks) > 100:
            for key in list(transfer_tasks.keys())[:-80]:
                transfer_tasks.pop(key, None)
    return task_id


def update_transfer_task(task_id, **updates):
    with transfer_lock:
        if task_id in transfer_tasks:
            transfer_tasks[task_id].update(updates)


def get_transfer_task(task_id):
    with transfer_lock:
        return transfer_tasks.get(task_id)


def create_lora_task(payload):
    task_id = uuid.uuid4().hex
    with lora_lock:
        lora_exchange_tasks[task_id] = {
            'id': task_id,
            'status': 'pending',
            'message': 'ç­‰å¾…å¯åŠ¨',
            'process_id': None,
            'result_file': None,
            'callback': payload.get('callback'),
            'created_at': int(time.time())
        }
    return task_id


def update_lora_task(task_id, **updates):
    with lora_lock:
        if task_id in lora_exchange_tasks:
            lora_exchange_tasks[task_id].update(updates)


def get_lora_task(task_id):
    with lora_lock:
        return lora_exchange_tasks.get(task_id)

# Authentication removed - allow anonymous training

# å¯åŠ¨è®­ç»ƒè¿›ç¨‹
def start_training_process(train_type, params, client_id=None):
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # ä½¿ç”¨è¯¦ç»†çš„æ—¶é—´æˆ³ä½œä¸ºè¿›ç¨‹IDå’Œæ—¥å¿—æ–‡ä»¶å
    process_id = time.strftime('%Y%m%d_%H%M%S')
    # æ„å»ºlogfileç›®å½•çš„ç»å¯¹è·¯å¾„
    log_dir = os.path.join(script_dir, '../logfile')
    log_dir = os.path.abspath(log_dir)
    log_file = os.path.join(log_dir, f"train_{train_type}_{process_id}.log")
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs(log_dir, exist_ok=True)
    
    # è·å–GPUæ•°é‡å‚æ•°ï¼Œå¦‚æœå­˜åœ¨ä¸”å¤§äº1ï¼Œåˆ™ä½¿ç”¨torchrunå¯åŠ¨å¤šå¡è®­ç»ƒ
    gpu_num = int(params.get('gpu_num', 0)) if 'gpu_num' in params else 0
    use_torchrun = HAS_TORCH and GPU_COUNT > 0 and gpu_num > 1
    
    try:
        from .dispatcher import build_command
    except ImportError:
        import sys as _sys
        import os as _os
        _sys.path.append(_os.path.dirname(_os.path.abspath(__file__)))
        from dispatcher import build_command
    cmd = build_command(train_type, params, gpu_num, use_torchrun)
    if cmd is None:
        return None
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    with open(log_file, 'w') as f:
        f.write(f"å¼€å§‹è®­ç»ƒ {train_type} è¿›ç¨‹\n")
        f.write(f"å‘½ä»¤: {' '.join(cmd)}\n\n")
    
    # å¯åŠ¨è¿›ç¨‹
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        cwd=os.path.dirname(os.path.abspath(__file__))
    )
    
    # å­˜å‚¨è¿›ç¨‹ä¿¡æ¯
    training_processes[process_id] = {
        'process': process,
        'train_type': train_type,
        'log_file': log_file,
        'start_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'start_timestamp': time.time(),  # æ·»åŠ æ—¶é—´æˆ³ç”¨äºè¿›åº¦è®¡ç®—
        'running': True,
        'error': False,
        'train_monitor': params.get('train_monitor', 'none'),  # ä¿å­˜è®­ç»ƒç›‘æ§è®¾ç½®
        'swanlab_url': None,
        'next_line_is_swanlab_url': False,
        'client_id': client_id
    }
    
    # å¼€å§‹è¯»å–è¾“å‡º
    def read_output():
        try:
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯swanlabé“¾æ¥çš„è¡Œ
                    output_stripped = output.strip()
                    if training_processes[process_id]['next_line_is_swanlab_url']:
                        # ä¿å­˜swanlabé“¾æ¥
                        training_processes[process_id]['swanlab_url'] = output_stripped
                        training_processes[process_id]['next_line_is_swanlab_url'] = False
                    elif 'swanlab: ğŸš€ View run at' in output_stripped:
                        # æ ‡è®°ä¸‹ä¸€è¡Œæ˜¯swanlabé“¾æ¥
                        training_processes[process_id]['next_line_is_swanlab_url'] = True
                    
                    with open(log_file, 'a') as f:
                        f.write(output)
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦æˆåŠŸç»“æŸ
            if process.returncode != 0:
                training_processes[process_id]['error'] = True
        finally:
            training_processes[process_id]['running'] = False
    
    # å¯åŠ¨çº¿ç¨‹è¯»å–è¾“å‡º
    threading.Thread(target=read_output, daemon=True).start()
    
    return process_id

# Flaskè·¯ç”±
@app.route('/')
def index():
    # ä¼ é€’GPUä¿¡æ¯åˆ°å‰ç«¯
    return render_template(
        'index.html',
        has_gpu=HAS_TORCH and GPU_COUNT > 0,
        gpu_count=GPU_COUNT,
        transfer_debug=TRANSFER_DEBUG
    )

@app.route('/healthz')
def healthz():
    try:
        return jsonify({'status': 'ok', 'gpu': GPU_COUNT, 'methods': SUPPORTED_METHODS}), 200
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/train', methods=['POST'])
def train():
    data = request.json
    train_type = data.get('train_type')
    
    # ç§»é™¤ä¸ç›¸å…³çš„å‚æ•°
    params = data.copy()
    
    # å¤„ç†å¤é€‰æ¡†å‚æ•°
    if 'from_resume' not in params:
        params['from_resume'] = '0'
    
    # å¯åŠ¨è®­ç»ƒè¿›ç¨‹ - å…è®¸åŒ¿åè®­ç»ƒï¼Œä¸ä¼ å…¥client_id
    process_id = start_training_process(train_type, params)
    
    if process_id:
        return jsonify({'success': True, 'process_id': process_id})
    else:
        return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è®­ç»ƒç±»å‹'})

# æµ‹è¯•ç«¯ç‚¹ - æ·»åŠ æ¨¡æ‹Ÿè®­ç»ƒè¿›ç¨‹
@app.route('/test/add_process', methods=['POST'])
def add_test_process():
    """æ·»åŠ ä¸€ä¸ªæµ‹è¯•è¿›ç¨‹ç”¨äºéªŒè¯è‡ªåŠ¨æ›´æ–°åŠŸèƒ½"""
    import subprocess
    import threading
    
    process_id = f"test_process_{int(time.time())}"
    
    # åˆ›å»ºæµ‹è¯•è®­ç»ƒå‘½ä»¤ - åŒ…å«stepè¿›åº¦å’Œæ–°çš„logæ ¼å¼
    test_command = [
        'python', '-c', '''
import time
import sys

print("2024-11-21 14:30:00 - Starting pretrain training")
sys.stdout.flush()
time.sleep(1)

print("2024-11-21 14:30:01 - Loading dataset from ../dataset/pretrain_hq.jsonl")
sys.stdout.flush()
time.sleep(1)

print("2024-11-21 14:30:02 - Model initialized with 108M parameters")
sys.stdout.flush()
time.sleep(2)

# æµ‹è¯•å•epochä½†å¤šstepçš„æƒ…å†µï¼Œä½¿ç”¨æ–°çš„logæ ¼å¼
print("2024-11-21 14:30:03 - Epoch:[1/1] Starting training")
sys.stdout.flush()
time.sleep(1)

total_steps = 20
for step in range(1, total_steps + 1):
    # æ¨¡æ‹Ÿstepè¿›åº¦ï¼Œä½¿ç”¨æ–°çš„æ ¼å¼
    if step % 5 == 0 or step == total_steps:
        print(f"2024-11-21 14:30:{4 + step} - Epoch:[1/1]({step}/{total_steps}) Processing")
        sys.stdout.flush()
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œä½¿ç”¨æ–°çš„æ ¼å¼
    loss = 4.5 - step * 0.1
    lr = 1e-4 * (0.95 ** step)
    if step % 3 == 0:
        print(f"2024-11-21 14:30:{4 + step} - Epoch:[1/1]({step}/{total_steps}) loss:{loss:.6f} lr:{lr:.2e} epoch_Time:{step * 5.5:.1f}min:")
        sys.stdout.flush()
    
    time.sleep(0.5)

print("2024-11-21 14:30:25 - Training completed successfully")
sys.stdout.flush()
        '''
    ]
    
    # å¯åŠ¨è¿›ç¨‹
    process = subprocess.Popen(
        test_command,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
        bufsize=1
    )
    
    # ä¿å­˜è¿›ç¨‹ä¿¡æ¯
    log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../logfile')
    log_dir = os.path.abspath(log_dir)
    os.makedirs(log_dir, exist_ok=True)
    
    training_processes[process_id] = {
        'process': process,
        'train_type': 'pretrain',
        'log_file': os.path.join(log_dir, f'{process_id}.log'),
        'start_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'start_timestamp': time.time(),
        'running': True,
        'error': False,
        'train_monitor': 'none',
        'swanlab_url': None
    }
    
    # å¯åŠ¨çº¿ç¨‹è¯»å–è¾“å‡ºå¹¶å†™å…¥æ—¥å¿—æ–‡ä»¶
    def read_output():
        try:
            log_file = training_processes[process_id]['log_file']
            with open(log_file, 'w') as f:
                for line in iter(process.stdout.readline, ''):
                    if line:
                        f.write(line)
                        f.flush()
            process.wait()
            training_processes[process_id]['running'] = False
            if process.returncode != 0:
                training_processes[process_id]['error'] = True
        except Exception as e:
            print(f"è¯»å–æµ‹è¯•è¿›ç¨‹è¾“å‡ºæ—¶å‡ºé”™: {e}")
            training_processes[process_id]['running'] = False
            training_processes[process_id]['error'] = True
    
    threading.Thread(target=read_output, daemon=True).start()
    
    return jsonify({
        'success': True,
        'process_id': process_id,
        'message': 'æµ‹è¯•è¿›ç¨‹å·²æ·»åŠ '
    })

@app.route('/processes')
def processes():
    result = []
    for process_id, info in training_processes.items():
        # ç¡®å®šçŠ¶æ€
        status = 'è¿è¡Œä¸­' if info['running'] else \
                'æ‰‹åŠ¨åœæ­¢' if 'manually_stopped' in info and info['manually_stopped'] else \
                'å‡ºé”™' if info['error'] else 'å·²å®Œæˆ'
        
        # è®¡ç®—è®­ç»ƒè¿›åº¦ä¿¡æ¯
        progress = calculate_training_progress(process_id, info)
                
        result.append({
            'id': process_id,
            'train_type': info['train_type'],
            'start_time': info['start_time'],
            'running': info['running'],
            'error': info['error'],
            'status': status,
            'train_monitor': info.get('train_monitor', 'none'),  # æ·»åŠ train_monitorå­—æ®µ
            'swanlab_url': info.get('swanlab_url'),  # æ·»åŠ swanlab_urlå­—æ®µ
            'progress': progress  # æ·»åŠ è¿›åº¦ä¿¡æ¯
        })
    return jsonify(result)

@app.route('/api/browse')
def browse_files():
    """
    æµè§ˆæœåŠ¡å™¨æ–‡ä»¶ç³»ç»Ÿ
    æ”¯æŒè¿œç¨‹æ–‡ä»¶é€‰æ‹©åŠŸèƒ½
    """
    try:
        # è·å–è¯·æ±‚çš„è·¯å¾„å‚æ•°
        path = request.args.get('path', './')
        
        # å®‰å…¨æ£€æŸ¥ï¼šé™åˆ¶è®¿é—®èŒƒå›´
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.abspath(os.path.join(script_dir, '..'))
        
        # è§£æè¯·æ±‚çš„è·¯å¾„
        if path.startswith('./'):
            # ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•
            full_path = os.path.abspath(os.path.join(project_root, path[2:]))
        elif path.startswith('/'):
            # ç»å¯¹è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨é¡¹ç›®ç›®å½•å†…
            full_path = os.path.abspath(path)
        else:
            # ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•
            full_path = os.path.abspath(os.path.join(project_root, path))
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„åœ¨é¡¹ç›®ç›®å½•å†…
        if not full_path.startswith(project_root):
            full_path = project_root
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(full_path):
            return jsonify({'error': 'è·¯å¾„ä¸å­˜åœ¨', 'path': path})
        
        # è·å–ç›®å½•å†…å®¹
        if os.path.isdir(full_path):
            items = []
            try:
                # åˆ—å‡ºç›®å½•å†…å®¹
                for item in sorted(os.listdir(full_path)):
                    item_path = os.path.join(full_path, item)
                    
                    # è·³è¿‡éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
                    if item.startswith('.') or item.startswith('__'):
                        continue
                    
                    try:
                        stat = os.stat(item_path)
                        items.append({
                            'name': item,
                            'path': item_path,  # è¿”å›ç»å¯¹è·¯å¾„
                            'relative_path': os.path.relpath(item_path, project_root),  # åŒæ—¶è¿”å›ç›¸å¯¹è·¯å¾„ç”¨äºæ˜¾ç¤º
                            'type': 'directory' if os.path.isdir(item_path) else 'file',
                            'size': stat.st_size if os.path.isfile(item_path) else 0,
                            'modified': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime))
                        })
                    except (OSError, PermissionError):
                        # è·³è¿‡æ— æ³•è®¿é—®çš„é¡¹ç›®
                        continue
                
                return jsonify({
                    'current_path': full_path,  # è¿”å›ç»å¯¹è·¯å¾„
                    'relative_path': os.path.relpath(full_path, project_root),  # ç›¸å¯¹è·¯å¾„ç”¨äºæ˜¾ç¤º
                    'absolute_path': full_path,
                    'items': items,
                    'parent': os.path.dirname(full_path) if full_path != project_root else None
                })
            except (OSError, PermissionError) as e:
                return jsonify({'error': f'æ— æ³•è®¿é—®ç›®å½•: {str(e)}', 'path': path})
        
        else:
            # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶ä¿¡æ¯
            stat = os.stat(full_path)
            return jsonify({
                'name': os.path.basename(full_path),
                'path': full_path,  # è¿”å›ç»å¯¹è·¯å¾„
                'relative_path': os.path.relpath(full_path, project_root),  # ç›¸å¯¹è·¯å¾„ç”¨äºæ˜¾ç¤º
                'type': 'file',
                'size': stat.st_size,
                'modified': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime))
            })
            
    except Exception as e:
        return jsonify({'error': f'æµè§ˆæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'})

@app.route('/api/quick-paths')
def quick_paths():
    """
    è¿”å›å¸¸ç”¨è·¯å¾„å¿«æ·æ–¹å¼
    """
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.abspath(os.path.join(script_dir, '..'))
        
        quick_paths = [
            {'name': 'é¡¹ç›®æ ¹ç›®å½•', 'path': './', 'type': 'directory'},
            {'name': 'æ•°æ®é›†ç›®å½•', 'path': './dataset', 'type': 'directory'},
            {'name': 'æ¨¡å‹æ£€æŸ¥ç‚¹', 'path': './checkpoints', 'type': 'directory'},
            {'name': 'æ—¥å¿—æ–‡ä»¶', 'path': './logfile', 'type': 'directory'}
        ]
        
        # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
        valid_paths = []
        for item in quick_paths:
            full_path = os.path.join(project_root, item['path'][2:] if item['path'].startswith('./') else item['path'])
            if os.path.exists(full_path):
                valid_paths.append(item)
        
        return jsonify({'paths': valid_paths})
        
    except Exception as e:
        return jsonify({'error': f'è·å–å¿«æ·è·¯å¾„æ—¶å‡ºé”™: {str(e)}'})


@app.route('/api/ping')
def api_ping():
    token_error = check_transfer_token(request)
    if token_error:
        return token_error
    return jsonify({
        'status': 'ok',
        'server': socket.gethostname(),
        'time': int(time.time())
    })


@app.route('/api/out-files')
def list_out_files():
    """åˆ—å‡ºoutç›®å½•ä¸‹å¯ä¼ è¾“çš„æ–‡ä»¶"""
    token_error = check_transfer_token(request)
    if token_error:
        return token_error
    ensure_out_dir()
    files = []
    for root, _, filenames in os.walk(OUT_DIR):
        for name in filenames:
            file_path = os.path.join(root, name)
            try:
                stat = os.stat(file_path)
                rel_path = os.path.relpath(file_path, OUT_DIR)
                files.append({
                    'name': rel_path,
                    'size': stat.st_size,
                    'modified': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime))
                })
            except OSError:
                continue
    files.sort(key=lambda x: x['name'])
    return jsonify({'base': OUT_DIR, 'files': files})


@app.route('/api/eval/weights')
def api_eval_weights():
    base_weights, lora_weights = list_weights()
    return jsonify({'base_weights': base_weights, 'lora_weights': lora_weights, 'out_dir': OUT_DIR})


@app.route('/api/dataset-files')
def list_dataset_files():
    """åˆ—å‡ºdatasetç›®å½•ä¸‹å¯ä¼ è¾“çš„æ•°æ®æ–‡ä»¶"""
    token_error = check_transfer_token(request)
    if token_error:
        return token_error
    ensure_dataset_dir()
    files = []
    for root, _, filenames in os.walk(DATASET_DIR):
        for name in filenames:
            file_path = os.path.join(root, name)
            try:
                stat = os.stat(file_path)
                rel_path = os.path.relpath(file_path, DATASET_DIR)
                files.append({
                    'name': rel_path,
                    'size': stat.st_size,
                    'modified': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime))
                })
            except OSError:
                continue
    files.sort(key=lambda x: x['name'])
    return jsonify({'base': DATASET_DIR, 'files': files})


@app.route('/api/transfer-status/<task_id>')
def transfer_status(task_id):
    task = get_transfer_task(task_id)
    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    return jsonify(task)


@app.route('/api/ping-remote', methods=['POST'])
def ping_remote():
    data = request.get_json(silent=True) or {}
    target_url = normalize_target_url(data.get('target_url'))
    token = data.get('token')
    if not target_url:
        return jsonify({'error': 'ç›®æ ‡åœ°å€ä¸èƒ½ä¸ºç©º'}), 400
    headers = {}
    if token:
        headers['X-Transfer-Token'] = token
    start = time.time()
    try:
        resp = requests.get(f"{target_url}/api/ping", headers=headers, timeout=5)
        latency = int((time.time() - start) * 1000)
        if resp.status_code != 200:
            return jsonify({'error': f'è¿œç¨‹è¿”å›{resp.status_code}', 'latency_ms': latency}), 502
        payload = {}
        try:
            payload = resp.json()
        except Exception:
            payload = {'raw': resp.text[:200]}
        return jsonify({'success': True, 'latency_ms': latency, 'remote': target_url, 'data': payload})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 502


@app.route('/api/remote-out-files', methods=['POST'])
def remote_out_files():
    data = request.get_json(silent=True) or {}
    target_url = normalize_target_url(data.get('target_url'))
    token = data.get('token')
    if not target_url:
        return jsonify({'error': 'ç›®æ ‡åœ°å€ä¸èƒ½ä¸ºç©º'}), 400
    headers = {}
    if token:
        headers['X-Transfer-Token'] = token
    try:
        resp = requests.get(f"{target_url}/api/out-files", headers=headers, timeout=10)
        if resp.status_code != 200:
            return jsonify({'error': f'è¿œç¨‹è¿”å›{resp.status_code}', 'remote': target_url}), 502
        return jsonify({'success': True, 'remote': target_url, 'data': resp.json()})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 502


@app.route('/api/upload-dataset-to-remote', methods=['POST'])
def upload_dataset_to_remote():
    data = request.get_json(silent=True) or {}
    target_url = normalize_target_url(data.get('target_url'))
    filename = data.get('filename')
    token = data.get('token')
    overwrite = str(data.get('overwrite', False)).lower() in ('1', 'true', 'yes', 'on')
    if not target_url or not filename:
        return jsonify({'error': 'ç›®æ ‡åœ°å€å’Œæ•°æ®æ–‡ä»¶å‡ä¸èƒ½ä¸ºç©º'}), 400
    safe_rel, file_path = safe_dataset_path(filename)
    if not file_path or not os.path.exists(file_path):
        return jsonify({'error': 'è¯·é€‰æ‹©datasetç›®å½•ä¸‹å­˜åœ¨çš„æ–‡ä»¶'}), 400

    task_id = create_transfer_task('upload-dataset', safe_rel, target_url)
    threading.Thread(
        target=upload_worker_dataset,
        args=(task_id, target_url, safe_rel, file_path, token, overwrite),
        daemon=True
    ).start()
    return jsonify({'task_id': task_id})


@app.route('/api/upload-to-remote', methods=['POST'])
def upload_to_remote():
    data = request.get_json(silent=True) or {}
    target_url = normalize_target_url(data.get('target_url'))
    filename = data.get('filename')
    token = data.get('token')
    overwrite = str(data.get('overwrite', False)).lower() in ('1', 'true', 'yes', 'on')
    if not target_url or not filename:
        return jsonify({'error': 'ç›®æ ‡åœ°å€å’Œæ–‡ä»¶åå‡ä¸èƒ½ä¸ºç©º'}), 400
    safe_rel, file_path = safe_out_path(filename)
    if not file_path or not os.path.exists(file_path):
        return jsonify({'error': 'è¯·é€‰æ‹©outç›®å½•ä¸‹å­˜åœ¨çš„æ–‡ä»¶'}), 400

    task_id = create_transfer_task('upload', safe_rel, target_url)
    threading.Thread(
        target=upload_worker,
        args=(task_id, target_url, safe_rel, file_path, token, overwrite),
        daemon=True
    ).start()
    return jsonify({'task_id': task_id})


@app.route('/api/receive-weight', methods=['POST'])
def receive_weight():
    token_error = check_transfer_token(request)
    if token_error:
        return token_error
    filename = request.headers.get('X-Filename') or request.args.get('filename')
    overwrite = request.args.get('overwrite', '0') in ('1', 'true', 'yes')
    safe_rel, dest_path = safe_out_path(filename)
    if not dest_path:
        return jsonify({'success': False, 'error': 'æ–‡ä»¶åæ— æ•ˆæˆ–è¶Šç•Œ'}), 400
    ensure_out_dir()
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    if os.path.exists(dest_path) and not overwrite:
        return jsonify({'success': False, 'error': 'æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯·å¼€å¯è¦†ç›–åå†è¯•'}), 409

    bytes_written = 0
    try:
        with open(dest_path, 'wb') as f:
            for chunk in iter(lambda: request.stream.read(1024 * 1024), b''):
                if not chunk:
                    break
                f.write(chunk)
                bytes_written += len(chunk)
        return jsonify({'success': True, 'saved_as': safe_rel, 'bytes': bytes_written})
    except Exception as e:
        print(f"æ¥æ”¶æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/receive-dataset', methods=['POST'])
def receive_dataset():
    token_error = check_transfer_token(request)
    if token_error:
        return token_error
    filename = request.headers.get('X-Filename') or request.args.get('filename')
    overwrite = request.args.get('overwrite', '0') in ('1', 'true', 'yes')
    safe_rel, dest_path = safe_dataset_path(filename)
    if not dest_path:
        return jsonify({'success': False, 'error': 'æ–‡ä»¶åæ— æ•ˆæˆ–è¶Šç•Œ'}), 400
    ensure_dataset_dir()
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    if os.path.exists(dest_path) and not overwrite:
        return jsonify({'success': False, 'error': 'æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯·å¼€å¯è¦†ç›–åå†è¯•'}), 409

    bytes_written = 0
    try:
        with open(dest_path, 'wb') as f:
            for chunk in iter(lambda: request.stream.read(1024 * 1024), b''):
                if not chunk:
                    break
                f.write(chunk)
                bytes_written += len(chunk)
        return jsonify({'success': True, 'saved_as': safe_rel, 'bytes': bytes_written})
    except Exception as e:
        print(f"æ¥æ”¶æ•°æ®é›†å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/download-weight')
def download_weight():
    token_error = check_transfer_token(request)
    if token_error:
        return token_error
    filename = request.args.get('filename')
    safe_rel, file_path = safe_out_path(filename)
    if not file_path or not os.path.exists(file_path):
        return jsonify({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    return send_file(file_path, as_attachment=True, download_name=os.path.basename(file_path))


@app.route('/api/pull-remote-weight', methods=['POST'])
def pull_remote_weight():
    data = request.get_json(silent=True) or {}
    source_url = normalize_target_url(data.get('source_url'))
    filename = data.get('filename')
    token = data.get('token')
    overwrite = str(data.get('overwrite', False)).lower() in ('1', 'true', 'yes', 'on')
    if not source_url or not filename:
        return jsonify({'error': 'æ¥æºåœ°å€å’Œæ–‡ä»¶åå‡ä¸èƒ½ä¸ºç©º'}), 400
    safe_rel, dest_path = safe_out_path(filename)
    if not dest_path:
        return jsonify({'error': 'æ— æ•ˆçš„æ–‡ä»¶å'}), 400

    task_id = create_transfer_task('download', safe_rel, source_url)
    threading.Thread(
        target=pull_worker,
        args=(task_id, source_url, safe_rel, dest_path, token, overwrite),
        daemon=True
    ).start()
    return jsonify({'task_id': task_id})


@app.route('/api/lora/start', methods=['POST'])
def lora_start():
    """åœ¨å½“å‰æœåŠ¡å™¨å¯åŠ¨LoRAè®­ç»ƒï¼ˆä¾›è¿œç¨‹è°ƒç”¨ï¼‰"""
    token_error = check_transfer_token(request, allow_body=True)
    if token_error:
        return token_error
    data = request.get_json(silent=True) or {}
    dataset_name = data.get('dataset')
    base_weight = data.get('base_weight')
    params = data.get('params') or {}
    callback_url = normalize_target_url(data.get('callback_url'))
    callback_token = data.get('callback_token')

    safe_dataset, dataset_path = safe_dataset_path(dataset_name)
    safe_weight, weight_path = safe_out_path(base_weight)
    if not dataset_path or not os.path.exists(dataset_path):
        return jsonify({'error': 'æ•°æ®é›†ä¸å­˜åœ¨æˆ–æ— æ•ˆ'}), 400
    if not weight_path or not os.path.exists(weight_path):
        return jsonify({'error': 'åŸºåº§æƒé‡ä¸å­˜åœ¨æˆ–æ— æ•ˆ'}), 400

    lora_name = params.get('lora_name') or 'remote_lora'
    hidden_size = int(params.get('hidden_size') or 512)
    # ç»“æœæ–‡ä»¶åæ¨æµ‹
    result_file = os.path.join(OUT_DIR, 'lora', f"{lora_name}_{hidden_size}.pth")
    os.makedirs(os.path.dirname(result_file), exist_ok=True)

    # æ„é€ è®­ç»ƒå‚æ•°
    train_params = {
        'train_type': 'lora',
        'data_path': dataset_path,
        'from_weight': params.get('from_weight') or derive_from_weight_prefix(safe_weight),
        'lora_name': lora_name,
        'save_dir': os.path.join(OUT_DIR, 'lora'),
        'epochs': params.get('epochs') or 10,
        'batch_size': params.get('batch_size') or 16,
        'learning_rate': params.get('learning_rate') or 1e-4,
        'hidden_size': hidden_size,
        'num_hidden_layers': params.get('num_hidden_layers') or 8,
        'max_seq_len': params.get('max_seq_len') or 512,
        'use_moe': params.get('use_moe') or 0,
        'log_interval': params.get('log_interval') or 10,
        'save_interval': params.get('save_interval') or 1,
        'from_resume': params.get('from_resume') or 0,
        'train_monitor': 'none'
    }

    process_id = start_training_process('lora', train_params)
    if not process_id:
        return jsonify({'error': 'æ— æ³•å¯åŠ¨LoRAè®­ç»ƒ'}), 500

    task_id = create_lora_task({'callback': {'url': callback_url, 'token': callback_token}})
    update_lora_task(task_id, status='running', message='LoRAè®­ç»ƒä¸­', process_id=process_id, result_file=os.path.relpath(result_file, OUT_DIR))
    threading.Thread(
        target=watch_lora_training,
        args=(task_id, process_id, result_file, {'url': callback_url, 'token': callback_token}),
        daemon=True
    ).start()

    return jsonify({
        'task_id': task_id,
        'process_id': process_id,
        'result_file': os.path.relpath(result_file, OUT_DIR)
    })


@app.route('/api/lora/status/<task_id>')
def lora_status(task_id):
    token_error = check_transfer_token(request)
    if token_error:
        return token_error
    task = get_lora_task(task_id)
    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    return jsonify(task)


@app.route('/api/lora/remote-start', methods=['POST'])
def lora_remote_start():
    data = request.get_json(silent=True) or {}
    target_url = normalize_target_url(data.get('target_url'))
    token = data.get('token')
    payload = data.get('payload') or {}
    if not target_url:
        return jsonify({'error': 'ç›®æ ‡åœ°å€ä¸èƒ½ä¸ºç©º'}), 400
    headers = {}
    if token:
        headers['X-Transfer-Token'] = token
    try:
        resp = requests.post(f"{target_url}/api/lora/start", json=payload, headers=headers, timeout=30)
        return jsonify({'success': resp.status_code == 200, 'remote_response': resp.json() if resp.content else {}, 'status_code': resp.status_code})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 502


@app.route('/api/lora/remote-status', methods=['POST'])
def lora_remote_status():
    data = request.get_json(silent=True) or {}
    target_url = normalize_target_url(data.get('target_url'))
    task_id = data.get('task_id')
    token = data.get('token')
    if not target_url or not task_id:
        return jsonify({'error': 'ç¼ºå°‘ç›®æ ‡åœ°å€æˆ–ä»»åŠ¡ID'}), 400
    headers = {}
    if token:
        headers['X-Transfer-Token'] = token
    try:
        resp = requests.get(f"{target_url}/api/lora/status/{task_id}", headers=headers, timeout=15)
        if resp.status_code != 200:
            return jsonify({'success': False, 'status_code': resp.status_code, 'remote': target_url})
        return jsonify({'success': True, 'data': resp.json()})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 502


def upload_worker(task_id, target_url, safe_rel, file_path, token=None, overwrite=False):
    update_transfer_task(task_id, status='running', progress=0, message='å‡†å¤‡ä¸Šä¼ ')
    try:
        file_size = os.path.getsize(file_path)
        headers = {
            'Content-Type': 'application/octet-stream',
            'X-Filename': os.path.basename(safe_rel)
        }
        if token:
            headers['X-Transfer-Token'] = token
        params = {'overwrite': '1' if overwrite else '0'}
        bytes_sent = 0

        def stream_file():
            nonlocal bytes_sent
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(1024 * 1024), b''):
                    bytes_sent += len(chunk)
                    if file_size:
                        progress = min(100, round(bytes_sent / file_size * 100, 2))
                    else:
                        progress = 100
                    update_transfer_task(task_id, status='running', progress=progress, message='ä¸Šä¼ ä¸­...')
                    yield chunk

        resp = requests.post(
            f"{target_url}/api/receive-weight",
            params=params,
            data=stream_file(),
            headers=headers,
            timeout=600
        )
        if resp.status_code != 200:
            update_transfer_task(task_id, status='error', message=f"è¿œç¨‹è¿”å›{resp.status_code}: {resp.text[:200]}")
            return
        update_transfer_task(task_id, status='success', progress=100, message='ä¸Šä¼ å®Œæˆ')
    except Exception as e:
        update_transfer_task(task_id, status='error', message=str(e))


def upload_worker_dataset(task_id, target_url, safe_rel, file_path, token=None, overwrite=False):
    update_transfer_task(task_id, status='running', progress=0, message='å‡†å¤‡ä¸Šä¼ æ•°æ®é›†')
    try:
        file_size = os.path.getsize(file_path)
        headers = {
            'Content-Type': 'application/octet-stream',
            'X-Filename': os.path.basename(safe_rel)
        }
        if token:
            headers['X-Transfer-Token'] = token
        params = {'overwrite': '1' if overwrite else '0'}
        bytes_sent = 0

        def stream_file():
            nonlocal bytes_sent
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(1024 * 1024), b''):
                    bytes_sent += len(chunk)
                    if file_size:
                        progress = min(100, round(bytes_sent / file_size * 100, 2))
                    else:
                        progress = 100
                    update_transfer_task(task_id, status='running', progress=progress, message='ä¸Šä¼ æ•°æ®é›†...')
                    yield chunk

        resp = requests.post(
            f"{target_url}/api/receive-dataset",
            params=params,
            data=stream_file(),
            headers=headers,
            timeout=600
        )
        if resp.status_code != 200:
            update_transfer_task(task_id, status='error', message=f"è¿œç¨‹è¿”å›{resp.status_code}: {resp.text[:200]}")
            return
        update_transfer_task(task_id, status='success', progress=100, message='æ•°æ®é›†ä¸Šä¼ å®Œæˆ')
    except Exception as e:
        update_transfer_task(task_id, status='error', message=str(e))


def pull_worker(task_id, source_url, safe_rel, dest_path, token=None, overwrite=False):
    update_transfer_task(task_id, status='running', progress=0, message='å‡†å¤‡ä¸‹è½½')
    ensure_out_dir()
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    if os.path.exists(dest_path) and not overwrite:
        update_transfer_task(task_id, status='error', message='æ–‡ä»¶å·²å­˜åœ¨ä¸”æœªå¼€å¯è¦†ç›–')
        return
    headers = {}
    if token:
        headers['X-Transfer-Token'] = token
    params = {'filename': safe_rel}
    try:
        with requests.get(
            f"{source_url}/api/download-weight",
            headers=headers,
            params=params,
            stream=True,
            timeout=600
        ) as resp:
            if resp.status_code != 200:
                try:
                    detail = resp.json()
                except Exception:
                    detail = {'raw': resp.text[:200]}
                update_transfer_task(task_id, status='error', message=f"è¿œç¨‹è¿”å›{resp.status_code}: {detail}")
                return
            total_size = int(resp.headers.get('Content-Length', 0)) if resp.headers.get('Content-Length') else 0
            bytes_written = 0
            with open(dest_path, 'wb') as f:
                for chunk in resp.iter_content(chunk_size=1024 * 1024):
                    if not chunk:
                        continue
                    f.write(chunk)
                    bytes_written += len(chunk)
                    if total_size:
                        progress = min(100, round(bytes_written / total_size * 100, 2))
                    else:
                        # å½“æ— æ³•è·å–æ€»å¤§å°æ—¶ï¼Œç”¨å†™å…¥é‡çš„è¿‘ä¼¼å€¼é©±åŠ¨è¿›åº¦ï¼Œä½†ä¿ç•™åˆ°99%
                        current = get_transfer_task(task_id) or {}
                        progress = min(99, current.get('progress', 0) + 1)
                    update_transfer_task(task_id, status='running', progress=progress, message='ä¸‹è½½ä¸­...')
            update_transfer_task(task_id, status='success', progress=100, message='ä¸‹è½½å®Œæˆ')
    except Exception as e:
        update_transfer_task(task_id, status='error', message=str(e))


def derive_from_weight_prefix(filename):
    """æ ¹æ®æ–‡ä»¶åæ¨æµ‹from_weightå‰ç¼€"""
    base = os.path.splitext(os.path.basename(filename))[0]
    # å°è¯•å»æ‰æœ«å°¾çš„ _æ•°å­— æˆ– _æ•°å­—_moe
    parts = base.split('_')
    if len(parts) >= 2 and parts[-1].isdigit():
        return '_'.join(parts[:-1])
    if len(parts) >= 3 and parts[-2].isdigit() and parts[-1].lower() == 'moe':
        return '_'.join(parts[:-2])
    return base


def push_weight_to_callback(callback_url, file_path, token=None):
    if not callback_url:
        return False, 'æ— å›ä¼ åœ°å€'
    try:
        headers = {
            'Content-Type': 'application/octet-stream',
            'X-Filename': os.path.basename(file_path)
        }
        if token:
            headers['X-Transfer-Token'] = token
        with open(file_path, 'rb') as f:
            resp = requests.post(
                f"{callback_url}/api/receive-weight",
                params={'overwrite': '1'},
                headers=headers,
                data=iter(lambda: f.read(1024 * 1024), b''),
                timeout=600
            )
        if resp.status_code == 200:
            return True, 'å›ä¼ æˆåŠŸ'
        return False, f"å›ä¼ å¤±è´¥: {resp.status_code}"
    except Exception as e:
        return False, str(e)


def load_eval_model(params):
    """æŒ‰eval_llm.pyé€»è¾‘åŠ è½½æ¨¡å‹"""
    load_from = params.get('load_from', 'model')
    save_dir = params.get('save_dir', 'out')
    weight = params.get('weight', 'full_sft')
    lora_weight = params.get('lora_weight', 'None')
    hidden_size = int(params.get('hidden_size', 512))
    num_hidden_layers = int(params.get('num_hidden_layers', 8))
    use_moe = int(params.get('use_moe', 0))
    inference_rope_scaling = bool(params.get('inference_rope_scaling', False))
    device = params.get('device') or ('cuda' if torch.cuda.is_available() else 'cpu')

    tokenizer = AutoTokenizer.from_pretrained(load_from)
    if 'model' in load_from:
        model = MiniMindForCausalLM(MiniMindConfig(
            hidden_size=hidden_size,
            num_hidden_layers=num_hidden_layers,
            use_moe=bool(use_moe),
            inference_rope_scaling=inference_rope_scaling
        ))
        moe_suffix = '_moe' if use_moe else ''
        ckp = os.path.abspath(os.path.join(PROJECT_ROOT, save_dir, f"{weight}_{hidden_size}{moe_suffix}.pth"))
        if not os.path.exists(ckp):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {ckp}")
        state = torch.load(ckp, map_location=device)
        model.load_state_dict(state, strict=True)
        if lora_weight and lora_weight != 'None':
            apply_lora(model)
            lora_path = os.path.abspath(os.path.join(PROJECT_ROOT, save_dir, 'lora', f"{lora_weight}_{hidden_size}.pth"))
            if not os.path.exists(lora_path):
                raise FileNotFoundError(f"æœªæ‰¾åˆ°LoRAæ–‡ä»¶: {lora_path}")
            load_lora(model, lora_path)
    else:
        model = AutoModelForCausalLM.from_pretrained(load_from, trust_remote_code=True)
    model.eval().to(device)
    return model, tokenizer, device


def run_eval_once(model, tokenizer, device, prompt, params, conversation=None):
    conversation = conversation or []
    conversation = conversation[-params['historys']:] if params['historys'] else []
    conversation.append({"role": "user", "content": prompt})

    templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
    if params['weight'] == 'reason':
        templates["enable_thinking"] = True
    inputs_text = tokenizer.apply_chat_template(**templates) if params['weight'] != 'pretrain' else (tokenizer.bos_token + prompt)
    inputs = tokenizer(inputs_text, return_tensors="pt", truncation=True).to(device)

    generated_ids = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=params['max_new_tokens'],
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        top_p=params['top_p'],
        temperature=params['temperature'],
        repetition_penalty=1.0
    )
    response = tokenizer.decode(generated_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
    conversation.append({"role": "assistant", "content": response})
    return response, conversation


@app.route('/api/eval/run', methods=['POST'])
def api_eval_run():
    data = request.get_json(silent=True) or {}
    try:
        params = {
            'load_from': data.get('load_from') or 'model',
            'save_dir': data.get('save_dir') or 'out',
            'weight': data.get('weight') or 'full_sft',
            'lora_weight': data.get('lora_weight') or 'None',
            'hidden_size': int(data.get('hidden_size') or 512),
            'num_hidden_layers': int(data.get('num_hidden_layers') or 8),
            'use_moe': int(data.get('use_moe') or 0),
            'inference_rope_scaling': bool(data.get('inference_rope_scaling') or False),
            'max_new_tokens': int(data.get('max_new_tokens') or 512),
            'temperature': float(data.get('temperature') or 0.85),
            'top_p': float(data.get('top_p') or 0.85),
            'historys': int(data.get('historys') or 0),
            'device': data.get('device') or ('cuda' if torch.cuda.is_available() else 'cpu')
        }
    except Exception as e:
        return jsonify({'error': f'å‚æ•°é”™è¯¯: {str(e)}'}), 400

    prompts = []
    if data.get('use_default_prompts'):
        prompts = [
            'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
            'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„',
            'è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
            'è§£é‡Šä¸€ä¸‹\"å…‰åˆä½œç”¨\"çš„åŸºæœ¬è¿‡ç¨‹',
            'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨',
            'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹',
            'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',
            'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ'
        ]
    else:
        prompt = data.get('prompt')
        if not prompt:
            return jsonify({'error': 'è¯·æä¾›promptæˆ–é€‰æ‹©é»˜è®¤æµ‹è¯•'}), 400
        prompts = [prompt]

    try:
        model, tokenizer, device = load_eval_model(params)
    except Exception as e:
        return jsonify({'error': f'æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}'}), 500

    outputs = []
    conversation = []
    try:
        for p in prompts:
            setup_seed(2026)
            resp, conversation = run_eval_once(model, tokenizer, device, p, params, conversation)
            outputs.append({'prompt': p, 'response': resp})
    except Exception as e:
        return jsonify({'error': f'æ¨ç†å¤±è´¥: {str(e)}'}), 500

    return jsonify({'success': True, 'outputs': outputs})

def watch_lora_training(task_id, process_id, result_file, callback):
    """ç›‘æ§LoRAä»»åŠ¡ï¼Œå®Œæˆåå¯é€‰å›ä¼ """
    while True:
        info = training_processes.get(process_id)
        if not info:
            update_lora_task(task_id, status='error', message='æ‰¾ä¸åˆ°è®­ç»ƒè¿›ç¨‹')
            return
        if not info.get('running'):
            if info.get('error'):
                update_lora_task(task_id, status='error', message='è®­ç»ƒå¤±è´¥')
                return
            # æ£€æŸ¥ç»“æœæ–‡ä»¶
            if not os.path.exists(result_file):
                update_lora_task(task_id, status='error', message='è®­ç»ƒå®Œæˆä½†æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶')
                return
            update_lora_task(task_id, status='success', message='è®­ç»ƒå®Œæˆ', result_file=os.path.relpath(result_file, OUT_DIR))
            # å¯é€‰å›ä¼ 
            if callback and callback.get('url'):
                ok, msg = push_weight_to_callback(callback.get('url'), result_file, callback.get('token'))
                update_lora_task(task_id, callback_result=msg, callback_success=ok)
            return
        time.sleep(3)

@app.route('/logs/<process_id>')
def logs(process_id):
    # ç›´æ¥ä»æœ¬åœ°logfileç›®å½•è¯»å–æ—¥å¿—æ–‡ä»¶
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ„å»ºlogfileç›®å½•çš„ç»å¯¹è·¯å¾„
    log_dir = os.path.join(script_dir, '../logfile')
    log_dir = os.path.abspath(log_dir)
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ—¥å¿—æ–‡ä»¶
    log_file = None
    if os.path.exists(log_dir):
        for filename in os.listdir(log_dir):
            if filename.endswith(f'{process_id}.log'):
                log_file = os.path.join(log_dir, filename)
                break
    
    if not log_file or not os.path.exists(log_file):
        return 'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤'
    
    try:
        # ä½¿ç”¨é«˜æ•ˆä¸”å¥å£®çš„æ–¹æ³•è¯»å–æ–‡ä»¶çš„æœ€å200è¡Œ
        def read_last_n_lines(file_path, n=200):
            # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–æ–‡ä»¶ï¼Œé¿å…ç¼–ç é—®é¢˜
            with open(file_path, 'rb') as f:
                # è·å–æ–‡ä»¶å¤§å°
                f.seek(0, os.SEEK_END)
                file_size = f.tell()
                
                # å¦‚æœæ–‡ä»¶å¾ˆå°ï¼Œç›´æ¥è¯»å–æ•´ä¸ªæ–‡ä»¶
                if file_size < 1024 * 1024:  # å°äº1MBçš„æ–‡ä»¶ç›´æ¥è¯»å–
                    f.seek(0)
                    content = f.read()
                    return process_content(content)
                
                # å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨ç¼“å†²è¯»å–æœ«å°¾éƒ¨åˆ†
                # ä¼°è®¡éœ€è¦è¯»å–çš„å­—èŠ‚æ•°ï¼ˆå‡è®¾æ¯è¡Œå¹³å‡100å­—èŠ‚ï¼‰
                buffer_size = n * 200  # ä¸ºäº†ä¿é™©ï¼Œè¯»å–æ›´å¤šå­—èŠ‚
                
                # å®šä½åˆ°é€‚å½“çš„ä½ç½®
                position = max(0, file_size - buffer_size)
                f.seek(position)
                
                # è¯»å–ç¼“å†²åŒºå†…å®¹
                buffer = f.read(file_size - position)
                
                # å¤„ç†ç¼“å†²åŒºå†…å®¹
                lines = process_content(buffer)
                
                # ç¡®ä¿æˆ‘ä»¬è·å–åˆ°å®Œæ•´çš„è¡Œ
                # å¦‚æœç¼“å†²åŒºä¸æ˜¯ä»æ–‡ä»¶å¼€å¤´å¼€å§‹ï¼Œç¬¬ä¸€ä¸ªè¡Œå¯èƒ½ä¸å®Œæ•´
                if position > 0:
                    # è·³è¿‡ç¬¬ä¸€ä¸ªå¯èƒ½ä¸å®Œæ•´çš„è¡Œ
                    if len(lines) > 1:
                        lines = lines[1:]
                    else:
                        # å¦‚æœåªæœ‰ä¸€è¡Œä¸”ä¸åœ¨æ–‡ä»¶å¼€å¤´ï¼Œå¯èƒ½éœ€è¦è¯»å–æ›´å¤š
                        # è¿™é‡Œç®€å•å¤„ç†ï¼Œç›´æ¥è¯»å–æ•´ä¸ªæ–‡ä»¶ï¼ˆç½•è§æƒ…å†µï¼‰
                        f.seek(0)
                        content = f.read()
                        lines = process_content(content)
                
                # è¿”å›æœ€ånè¡Œ
                return lines[-n:] if len(lines) > n else lines
        
        def process_content(content):
            # å°è¯•å¤šç§ç¼–ç æ–¹å¼è§£ç å†…å®¹
            encodings = ['utf-8', 'latin-1', 'gbk', 'gb2312']
            for encoding in encodings:
                try:
                    text = content.decode(encoding)
                    # ä½¿ç”¨Trueå‚æ•°ä¿ç•™æ¢è¡Œç¬¦ï¼Œç¡®ä¿è¡Œåˆ†éš”ç¬¦æ­£ç¡®
                    return text.splitlines(True)
                except UnicodeDecodeError:
                    continue
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯æ›¿æ¢æ¨¡å¼
            text = content.decode('utf-8', errors='replace')
            return text.splitlines(True)
        
        # è¯»å–æœ€å200è¡Œ
        last_200_lines = read_last_n_lines(log_file, 200)
        
        # ç¡®ä¿è¿”å›çš„å†…å®¹é¡ºåºæ­£ç¡®ï¼Œå¹¶ä¸”ä¸åŒ…å«ç©ºè¡Œ
        return ''.join(last_200_lines)
    except Exception as e:
        return f'è¯»å–æ—¥å¿—å¤±è´¥: {str(e)}'

@app.route('/logfiles')
def get_logfiles():
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ„å»ºlogfileç›®å½•çš„ç»å¯¹è·¯å¾„
    log_dir = os.path.join(script_dir, '../logfile')
    log_dir = os.path.abspath(log_dir)
    
    logfiles = []
    # è·å–æ‰€æœ‰è¿›ç¨‹IDç”¨äºå…³è”
    process_pids = set(training_processes.keys())
    
    if os.path.exists(log_dir):
        for filename in os.listdir(log_dir):
            if filename.endswith('.log') and filename.startswith('train_'):
                file_path = os.path.join(log_dir, filename)
                try:
                    modified_time = os.path.getmtime(file_path)
                    formatted_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modified_time))
                    # æå–è¿›ç¨‹ID
                    pid = filename.split('.')[-2].split('_')[-1] if filename.endswith('.log') else None
                    logfiles.append({
                        'filename': filename,
                        'modified_time': formatted_time,
                        'size': os.path.getsize(file_path),
                        'process_id': pid,
                        'has_process': pid in process_pids
                    })
                except Exception as e:
                    continue
    # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    logfiles.sort(key=lambda x: x['modified_time'], reverse=True)
    return jsonify(logfiles)

@app.route('/logfile-content/<filename>')
def get_logfile_content(filename):
    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åä¸åŒ…å«è·¯å¾„éå†å­—ç¬¦
    if '..' in filename or '/' in filename or '\\' in filename:
        return jsonify({'error': 'Invalid filename'}), 400
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ„å»ºlogfileç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œtrain_web_ui.pyåœ¨scriptsç›®å½•ä¸‹
    log_dir = os.path.join(script_dir, '../logfile')
    log_dir = os.path.abspath(log_dir)
    log_file = os.path.join(log_dir, filename)
    
    try:
        # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–æ–‡ä»¶ï¼Œå¯ä»¥æ›´å¯é åœ°ä¿ç•™åŸå§‹æ¢è¡Œç¬¦
        with open(log_file, 'rb') as f:
            content_bytes = f.read()
        
        # å°è¯•å¤šç§ç¼–ç æ–¹å¼è§£ç ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†æ¢è¡Œç¬¦
        encodings = ['utf-8', 'latin-1', 'gbk', 'gb2312']
        content = None
        
        for encoding in encodings:
            try:
                # è§£ç æ–‡ä»¶å†…å®¹ï¼Œä¿ç•™åŸå§‹æ¢è¡Œç¬¦
                content = content_bytes.decode(encoding)
                break
            except UnicodeDecodeError:
                continue
        
        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨errors='replace'å‚æ•°å¤„ç†ä¸å¯è§£ç çš„å­—ç¬¦
        if content is None:
            content = content_bytes.decode('utf-8', errors='replace')
        
        # ç¡®ä¿è¿”å›çš„å†…å®¹æ­£ç¡®ä¿ç•™æ‰€æœ‰æ¢è¡Œç¬¦
        return content
    except FileNotFoundError:
        return jsonify({'error': 'Log file not found'}), 404
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/delete-logfile/<filename>', methods=['DELETE'])
def delete_logfile(filename):
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ„å»ºlogfileç›®å½•çš„ç»å¯¹è·¯å¾„
    log_dir = os.path.join(script_dir, '../logfile')
    log_dir = os.path.abspath(log_dir)
    
    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    if '..' in filename or '/' in filename or '\\' in filename:
        return jsonify({'success': False, 'message': 'éæ³•çš„æ–‡ä»¶å'})
    
    log_file = os.path.join(log_dir, filename)
    if os.path.exists(log_file) and os.path.isfile(log_file):
        try:
            os.remove(log_file)
            return jsonify({'success': True, 'message': 'æ—¥å¿—æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        except Exception as e:
            print(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}")
            return jsonify({'success': False, 'message': f'åˆ é™¤å¤±è´¥: {str(e)}'})
    return jsonify({'success': False, 'message': 'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨'})


@app.route('/stop/<process_id>', methods=['POST'])
def stop(process_id):
    if process_id in training_processes and training_processes[process_id]['running']:
        process = training_processes[process_id]['process']
        # åœ¨Windowsä¸Šä½¿ç”¨terminateï¼Œåœ¨Unixä¸Šå°è¯•ä¼˜é›…ç»ˆæ­¢
        try:
            process.terminate()
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            process.wait(timeout=5)
            # æ ‡è®°ä¸ºæ‰‹åŠ¨åœæ­¢
            training_processes[process_id]['running'] = False
            training_processes[process_id]['manually_stopped'] = True
        except subprocess.TimeoutExpired:
            # å¦‚æœè¶…æ—¶ï¼Œå¼ºåˆ¶æ€æ­»
            process.kill()
            # æ ‡è®°ä¸ºæ‰‹åŠ¨åœæ­¢
            training_processes[process_id]['running'] = False
            training_processes[process_id]['manually_stopped'] = True
        return jsonify({'success': True})
    return jsonify({'success': False})

@app.route('/delete/<process_id>', methods=['POST'])
def delete(process_id):
    if process_id in training_processes:
        # ç¡®ä¿è¿›ç¨‹å·²ç»åœæ­¢
        if training_processes[process_id]['running']:
            # å¦‚æœè¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢å®ƒ
            try:
                process = training_processes[process_id]['process']
                process.terminate()
                try:
                    process.wait(timeout=3)
                except subprocess.TimeoutExpired:
                    process.kill()
            except Exception as e:
                print(f"åœæ­¢è¿›ç¨‹å¤±è´¥: {str(e)}")
        
        # ä»è¿›ç¨‹å­—å…¸ä¸­åˆ é™¤
        del training_processes[process_id]
        
        # å¯é€‰ï¼šåˆ é™¤å¯¹åº”çš„æ—¥å¿—æ–‡ä»¶
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            log_dir = os.path.join(script_dir, '../logfile')
            log_dir = os.path.abspath(log_dir)
            
            if os.path.exists(log_dir):
                for filename in os.listdir(log_dir):
                    if filename.endswith(f'{process_id}.log'):
                        os.remove(os.path.join(log_dir, filename))
        except Exception as e:
            print(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        return jsonify({'success': True})
    return jsonify({'success': False})

def find_available_port(preferred=None, allowed_ports=None):
    """åœ¨é™å®šç«¯å£ä¸­æŸ¥æ‰¾å¯ç”¨ç«¯å£ï¼ˆautodl ä»…å¼€æ”¾ 6006/6008ï¼‰"""
    allowed = allowed_ports or ALLOWED_PORTS
    ordered = []
    if preferred and preferred in allowed:
        ordered.append(preferred)
    ordered.extend([p for p in allowed if p not in ordered])
    for port in ordered:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('localhost', port))
        sock.close()
        if result != 0:
            return port
    return None

def save_processes_info():
    """ä¿å­˜è®­ç»ƒè¿›ç¨‹ä¿¡æ¯åˆ°æ–‡ä»¶"""
    try:
        # åˆ›å»ºä¸€ä¸ªä¸åŒ…å«è¿›ç¨‹å¯¹è±¡çš„å¯åºåˆ—åŒ–ç‰ˆæœ¬
        serializable_processes = {}
        for pid, info in training_processes.items():
            serializable_processes[pid] = {
                'pid': info.get('pid', info.get('process').pid) if isinstance(info.get('process'), subprocess.Popen) else info.get('pid'),
                'train_type': info['train_type'],
                'log_file': info['log_file'],
                'start_time': info['start_time'],
                'running': info['running'],
                'error': info.get('error', False),
                'manually_stopped': info.get('manually_stopped', False),
                'train_monitor': info.get('train_monitor', 'none'),  # ä¿å­˜train_monitor
                'swanlab_url': info.get('swanlab_url'),
                'client_id': info.get('client_id')
            }
        
        with open(PROCESSES_FILE, 'w', encoding='utf-8') as f:
            json.dump(serializable_processes, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜è¿›ç¨‹ä¿¡æ¯å¤±è´¥: {str(e)}")

def load_processes_info():
    """ä»æ–‡ä»¶åŠ è½½è®­ç»ƒè¿›ç¨‹ä¿¡æ¯"""
    global training_processes
    try:
        if os.path.exists(PROCESSES_FILE):
            with open(PROCESSES_FILE, 'r', encoding='utf-8') as f:
                loaded_processes = json.load(f)
            
            # æ£€æŸ¥æ¯ä¸ªè¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            for pid, info in loaded_processes.items():
                # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„å­—æ®µéƒ½å­˜åœ¨
                if 'swanlab_url' not in info:
                    info['swanlab_url'] = None
                if 'manually_stopped' not in info:
                    info['manually_stopped'] = False
                if 'error' not in info:
                    info['error'] = False
                if 'train_monitor' not in info:
                    info['train_monitor'] = 'none'
                if 'client_id' not in info:
                    info['client_id'] = None
                
                if info['running']:
                    try:
                        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                        proc = psutil.Process(info['pid'])
                        if proc.is_running() and proc.status() != 'zombie':
                            # è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œæ¢å¤ä¿¡æ¯
                            training_processes[pid] = info
                        else:
                            # è¿›ç¨‹å·²åœæ­¢
                            info['running'] = False
                            # å¦‚æœè¿›ç¨‹æœªè¢«æ˜ç¡®æ ‡è®°ä¸ºå®Œæˆæˆ–å‡ºé”™ï¼Œåˆ™é»˜è®¤ä¸ºæ‰‹åŠ¨åœæ­¢
                            if not info['error']:
                                info['manually_stopped'] = True
                            training_processes[pid] = info
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        # è¿›ç¨‹ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®
                        info['running'] = False
                        # å¦‚æœè¿›ç¨‹æœªè¢«æ˜ç¡®æ ‡è®°ä¸ºå®Œæˆæˆ–å‡ºé”™ï¼Œåˆ™é»˜è®¤ä¸ºæ‰‹åŠ¨åœæ­¢
                        if not info['error']:
                            info['manually_stopped'] = True
                        training_processes[pid] = info
                else:
                    # è¿›ç¨‹å·²åœæ­¢ï¼Œç›´æ¥æ¢å¤
                    training_processes[pid] = info
    except Exception as e:
        print(f"åŠ è½½è¿›ç¨‹ä¿¡æ¯å¤±è´¥: {str(e)}")

def handle_exit(signum, frame):
    """å¤„ç†ç¨‹åºé€€å‡ºä¿¡å·ï¼Œä¿å­˜è¿›ç¨‹ä¿¡æ¯"""
    print("æ­£åœ¨ä¿å­˜è¿›ç¨‹ä¿¡æ¯...  save at 'trainer_web/training_processes.json'...")
    save_processes_info()
    # åˆ é™¤PIDæ–‡ä»¶
    if os.path.exists(PID_FILE):
        try:
            os.remove(PID_FILE)
        except:
            pass
    sys.exit(0)

# æ³¨å†Œé€€å‡ºå¤„ç†å™¨
signal.signal(signal.SIGINT, handle_exit)  # Ctrl+C
if hasattr(signal, 'SIGTERM'):
    signal.signal(signal.SIGTERM, handle_exit)  # ç»ˆæ­¢ä¿¡å·

# æ³¨å†Œç¨‹åºé€€å‡ºæ—¶çš„å¤„ç†å‡½æ•°
atexit.register(save_processes_info)

if __name__ == '__main__':
    # åŠ è½½å·²ä¿å­˜çš„è¿›ç¨‹ä¿¡æ¯
    load_processes_info()
    
    # åˆ›å»ºPIDæ–‡ä»¶ï¼Œç”¨äºæ ‡è¯†webè¿›ç¨‹
    with open(PID_FILE, 'w') as f:
        f.write(str(os.getpid()))
    
    # autodl ä»…å¼€æ”¾ 6006/6008ï¼›ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ FLASK_PORT/PORT
    preferred_env_port = os.environ.get('FLASK_PORT') or os.environ.get('PORT')
    preferred_env_port = int(preferred_env_port) if preferred_env_port and str(preferred_env_port).isdigit() else None
    port = find_available_port(preferred=preferred_env_port)
    if port is not None:
        print(f"å¯åŠ¨FlaskæœåŠ¡å™¨åœ¨ http://0.0.0.0:{port}")
        print(f"ä½¿ç”¨nohupå¯åŠ¨å¯ä¿æŒæœåŠ¡æŒç»­è¿è¡Œ: nohup python -u scripts/train_web_ui.py &")
        # ä½¿ç”¨0.0.0.0ä½œä¸ºhostä»¥å…¼å®¹VSCodeçš„ç«¯å£è½¬å‘åŠŸèƒ½
        app.run(host='0.0.0.0', port=port, debug=False)  # ç”Ÿäº§ç¯å¢ƒå…³é—­debug
    else:
        print(f"æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ï¼Œè¯·æ£€æŸ¥ {ALLOWED_PORTS} æ˜¯å¦è¢«å ç”¨")
        # åˆ é™¤PIDæ–‡ä»¶
        if os.path.exists(PID_FILE):
            try:
                os.remove(PID_FILE)
            except:
                pass
        sys.exit(1)
# Registration endpoint removed - allow anonymous training
